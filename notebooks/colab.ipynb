{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuX33W-1St9"
      },
      "source": [
        "# Setting up the project in google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryHVUn4A1SuA"
      },
      "outputs": [],
      "source": [
        "# Cloning repository into current folder\n",
        "!git clone https://github.com/andraspalasti/deeplearning-hw.git\n",
        "!mv deeplearning-hw/* .\n",
        "!rm -rf deeplearning-hw/\n",
        "\n",
        "# Install the packages used\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQwt3iWh1SuC"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "data_dir = Path('data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HDab2C1SuD"
      },
      "source": [
        "## Download raw dataset\n",
        "\n",
        "To download the dataset from kaggle you need to be signed in.\n",
        "\n",
        "What do these cells do?\n",
        "1. Download raw dataset from kaggle\n",
        "1. Unzip the downloaded dataset\n",
        "1. Divide dataset into train, val, test datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOvySv1R6bmr"
      },
      "outputs": [],
      "source": [
        "#Set the enviroment variables for authentication\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"palstiandrs\"\n",
        "os.environ['KAGGLE_KEY'] = \"fbdfe3ac6bdf77c68b2c3da0e8dedd47\"\n",
        "\n",
        "# Download the dataset\n",
        "!mkdir -p data/raw/\n",
        "!kaggle competitions download -c airbus-ship-detection -p data/raw/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDm9GE19iZcD"
      },
      "outputs": [],
      "source": [
        "# Unzipping the downloaded data\n",
        "!unzip -u -d data/raw/ data/raw/airbus-ship-detection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3W6Q082XeXP"
      },
      "outputs": [],
      "source": [
        "!echo \"Number of images in raw dataset: $(ls -l data/raw/train_v2/ | wc -l)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU2vH1VS7vDX"
      },
      "outputs": [],
      "source": [
        "from math import floor\n",
        "\n",
        "# Limit the number of images that we use to 100,000\n",
        "# othwerise dataset would be too big\n",
        "num_images = len(list((data_dir / 'raw' / 'train_v2').glob('*.jpg')))\n",
        "num_images = min(num_images, 100_000)\n",
        "\n",
        "train_size = floor(num_images * 0.8)\n",
        "val_size = floor(num_images * 0.1)\n",
        "test_size = floor(num_images * 0.1)\n",
        "num_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM-xvETz1SuE"
      },
      "outputs": [],
      "source": [
        "# Split training images into train, val, test sets using images from the raw dataset\n",
        "!mkdir -p data/processed/\n",
        "\n",
        "# Create training dataset\n",
        "!mkdir -p data/processed/train/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" | head -n {train_size} | tr '\\n' '\\0' \\\n",
        "    | xargs -0 mv -t data/processed/train/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/train_ship_segmentations.csv\n",
        "\n",
        "# Create validation dataset\n",
        "!mkdir -p data/processed/val/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" | head -n {val_size} \\\n",
        "    | tr '\\n' '\\0' | xargs -0 mv -t data/processed/val/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/val_ship_segmentations.csv\n",
        "\n",
        "# Create test dataset\n",
        "!mkdir -p data/processed/test/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" | head -n {test_size} \\\n",
        "    | tr '\\n' '\\0' | xargs -0 mv -t data/processed/test/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/test_ship_segmentations.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itDI-T3R1SuF"
      },
      "outputs": [],
      "source": [
        "from src.data import filter_missing\n",
        "proc_dir = data_dir / 'processed'\n",
        "\n",
        "# Filter missing annotations\n",
        "for dataset in ['train', 'val', 'test']:\n",
        "    filter_missing(proc_dir / f'{dataset}_ship_segmentations.csv',\n",
        "                   proc_dir / f'{dataset}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!echo \"Number of images in dataset: $(find data/processed/*/ -name \"*.jpg\" | wc -l)\"\n",
        "!echo \"Size of dataset on disk: $(du -sh data/processed)\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
