{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andraspalasti/deeplearning-hw/blob/main/notebooks/data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuX33W-1St9"
      },
      "source": [
        "## Setting up the project in google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryHVUn4A1SuA"
      },
      "outputs": [],
      "source": [
        "# Cloning repository into current folder\n",
        "!git clone https://github.com/andraspalasti/deeplearning-hw.git\n",
        "!mv deeplearning-hw/* .\n",
        "!rm -rf deeplearning-hw/\n",
        "\n",
        "# Install the packages used\n",
        "%pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HDab2C1SuD"
      },
      "source": [
        "## Preparing raw dataset\n",
        "\n",
        "To download the dataset from kaggle you need to be signed in, to sign in fill in the credentials listed in the cell below.\n",
        "\n",
        "What do these cells do?\n",
        "1. Download raw dataset from kaggle\n",
        "1. Examin the ratio of images\n",
        "1. Unzip the selected part of the raw dataset\n",
        "1. Divide dataset into train, val, test datasets\n",
        "1. Optionally save the dataset into google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l43qpuIM5Cbc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the enviroment variables for authentication\n",
        "if 'KAGGLE_USERNAME' not in os.environ:\n",
        "    os.environ['KAGGLE_USERNAME'] = \"xxxx\"\n",
        "    os.environ['KAGGLE_KEY'] = \"xxxx\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from src.data import download_dataset, filter_missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aqFx0Y275Cbc"
      },
      "outputs": [],
      "source": [
        "# Set up directories to work in\n",
        "data_dir = Path('data')\n",
        "if not data_dir.exists():\n",
        "    data_dir.mkdir()\n",
        "\n",
        "raw_dir = data_dir / 'raw'\n",
        "if not raw_dir.exists():\n",
        "    raw_dir.mkdir()\n",
        "\n",
        "proc_dir = data_dir / 'processed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EOvySv1R6bmr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading airbus-ship-detection.zip to data/raw\n",
            "... resuming from 277872640 bytes (30412638106 bytes left) ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.6G/28.6G [14:53<00:00, 34.1MB/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "data/raw/airbus-ship-detection.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download dataset returns the downloaded zip file's location\n",
        "dataset_path = download_dataset(raw_dir)\n",
        "print(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XD_i2a6W5Cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 192556 number of images in the dataset\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EncodedPixels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageId</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>00003e153.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0001124c7.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000155de5.jpg</th>\n",
              "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000194a2d.jpg</th>\n",
              "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0001b1832.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fffedbb6b.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ffff2aa57.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ffff6e525.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ffffc50b4.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ffffe97f3.jpg</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>192556 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   EncodedPixels\n",
              "ImageId                                                         \n",
              "00003e153.jpg                                                   \n",
              "0001124c7.jpg                                                   \n",
              "000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
              "000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n",
              "0001b1832.jpg                                                   \n",
              "...                                                          ...\n",
              "fffedbb6b.jpg                                                   \n",
              "ffff2aa57.jpg                                                   \n",
              "ffff6e525.jpg                                                   \n",
              "ffffc50b4.jpg                                                   \n",
              "ffffe97f3.jpg                                                   \n",
              "\n",
              "[192556 rows x 1 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zip = ZipFile(dataset_path)\n",
        "\n",
        "# Export csv file containing segmentations\n",
        "csv_path = Path(zip.extract('train_ship_segmentations_v2.csv', raw_dir))\n",
        "segmentations = pd.read_csv(csv_path)\n",
        "segmentations['EncodedPixels'] = segmentations['EncodedPixels'].fillna('')\n",
        "segmentations = segmentations.groupby('ImageId').agg({'EncodedPixels': ' '.join})\n",
        "\n",
        "print(f'There are {len(segmentations)} number of images in the dataset')\n",
        "segmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gxdkiXiF5Cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 42556 images that contain ships\n",
            "Ratio of images containing ships and all images: 22.10%\n"
          ]
        }
      ],
      "source": [
        "imgs_with_ships = segmentations[segmentations['EncodedPixels'] != '']\n",
        "print(f'There are {len(imgs_with_ships)} images that contain ships')\n",
        "\n",
        "ratio = len(imgs_with_ships) / len(segmentations)\n",
        "print(f'Ratio of images containing ships and all images: {ratio*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bBZ8vf95Cbc"
      },
      "source": [
        "We have a lot of images that do not contain ships so for now they have\n",
        "less value for us. We are going to use 60000 images to create our own\n",
        "dataset (using all the images would be too much for us anyway). In\n",
        "the 60000 images we will put all of the images that contain ships and\n",
        "for the rest we will use images that do not contain ships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WklrNdp-5Cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42556 number of images contain ships\n",
            "17444 number of images do not contain ships\n"
          ]
        }
      ],
      "source": [
        "dataset_size = 60_000\n",
        "\n",
        "image_ids = list(imgs_with_ships.index)\n",
        "print(f'{len(image_ids)} number of images contain ships')\n",
        "\n",
        "# Fill the rest of the dataset with images that do not contain ships\n",
        "print(f'{dataset_size-len(image_ids)} number of images do not contain ships')\n",
        "imgs_without_ships = segmentations[segmentations['EncodedPixels'] == '']\n",
        "image_ids.extend(imgs_without_ships[:dataset_size-len(image_ids)].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o6fXWiEY5Cbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting: 1dbeec1ea.jpg: 100%|██████████| 60000/60000 [01:40<00:00, 598.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# Our next task will be to only extract the images that are in our dataset\n",
        "for image_id in (t := tqdm(image_ids)):\n",
        "    zip.extract(f'train_v2/{image_id}', path=raw_dir)\n",
        "    t.set_description(f'Extracting: {image_id}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s3W6Q082XeXP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in raw dataset: 60000\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in raw dataset: $(ls -1 data/raw/train_v2/ | wc -l)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DU2vH1VS7vDX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full size of dataset: 60000\n",
            "\tTrain set size: 54000\n",
            "\tValidation set size: 3000\n",
            "\tTest set size: 3000\n"
          ]
        }
      ],
      "source": [
        "from math import floor\n",
        "\n",
        "train_size = floor(dataset_size * 0.9)\n",
        "val_size = floor(dataset_size * 0.05)\n",
        "test_size = floor(dataset_size * 0.05)\n",
        "print(f'Full size of dataset: {dataset_size}')\n",
        "print(f'\\tTrain set size: {train_size}')\n",
        "print(f'\\tValidation set size: {val_size}')\n",
        "print(f'\\tTest set size: {test_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NM-xvETz1SuE"
      },
      "outputs": [],
      "source": [
        "# Split training images into train, val, test sets using images from the raw dataset\n",
        "!mkdir -p data/processed/\n",
        "\n",
        "# Create training dataset\n",
        "!mkdir -p data/processed/train/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" \\\n",
        "    | sort -R \\\n",
        "    | head -n {train_size} \\\n",
        "    | tr '\\n' '\\0' \\\n",
        "    | xargs -0 mv -t data/processed/train/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/train_ship_segmentations.csv\n",
        "\n",
        "# Create validation dataset\n",
        "!mkdir -p data/processed/val/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" \\\n",
        "    | sort -R \\\n",
        "    | head -n {val_size} \\\n",
        "    | tr '\\n' '\\0' \\\n",
        "    | xargs -0 mv -t data/processed/val/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/val_ship_segmentations.csv\n",
        "\n",
        "# Create test dataset\n",
        "!mkdir -p data/processed/test/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" \\\n",
        "    | sort -R \\\n",
        "    | head -n {test_size} \\\n",
        "    | tr '\\n' '\\0' \\\n",
        "    | xargs -0 mv -t data/processed/test/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/test_ship_segmentations.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "itDI-T3R1SuF"
      },
      "outputs": [],
      "source": [
        "# Filter missing annotations\n",
        "for dataset_path in ['train', 'val', 'test']:\n",
        "    filter_missing(proc_dir / f'{dataset_path}_ship_segmentations.csv',\n",
        "                   proc_dir / f'{dataset_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uapCp-Yk5Cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in dataset: 60000\n",
            "Size of dataset on disk: 8.7G\tdata/processed\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in dataset: $(find data/processed/*/ -name \"*.jpg\" | wc -l)\"\n",
        "!echo \"Size of dataset on disk: $(du -sh data/processed)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rdEA1nzN5Cbd"
      },
      "outputs": [],
      "source": [
        "# Optional save dataset into google drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    !zip -r gdrive/MyDrive/airbus-dataset.zip data/processed/*\n",
        "    drive.flush_and_unmount()\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
