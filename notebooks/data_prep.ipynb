{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andraspalasti/deeplearning-hw/blob/main/notebooks/data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuX33W-1St9"
      },
      "source": [
        "## Setting up the project in google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryHVUn4A1SuA"
      },
      "outputs": [],
      "source": [
        "# Cloning repository into current folder\n",
        "!git clone https://github.com/andraspalasti/deeplearning-hw.git\n",
        "!mv deeplearning-hw/* .\n",
        "!rm -rf deeplearning-hw/\n",
        "\n",
        "# Install the packages used\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HDab2C1SuD"
      },
      "source": [
        "## Preparing raw dataset\n",
        "\n",
        "To download the dataset from kaggle you need to be signed in, to sign in fill in the credentials listed in the cell below.\n",
        "\n",
        "What do these cells do?\n",
        "1. Download raw dataset from kaggle\n",
        "1. Examin the ratio of images\n",
        "1. Unzip the selected part of the raw dataset\n",
        "1. Divide dataset into train, val, test datasets\n",
        "1. Optionally save the dataset into google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l43qpuIM5Cbc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the enviroment variables for authentication\n",
        "if 'KAGGLE_USERNAME' not in os.environ:\n",
        "    os.environ['KAGGLE_USERNAME'] = \"xxxx\"\n",
        "    os.environ['KAGGLE_KEY'] = \"xxxx\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from src.data import download_dataset, filter_missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqFx0Y275Cbc"
      },
      "outputs": [],
      "source": [
        "# Set up directories to work in\n",
        "data_dir = Path('data')\n",
        "if not data_dir.exists():\n",
        "    data_dir.mkdir()\n",
        "\n",
        "raw_dir = data_dir / 'raw'\n",
        "if not raw_dir.exists():\n",
        "    raw_dir.mkdir()\n",
        "\n",
        "proc_dir = data_dir / 'processed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOvySv1R6bmr"
      },
      "outputs": [],
      "source": [
        "# Download dataset returns the downloaded zip file's location\n",
        "dataset_path = download_dataset(raw_dir)\n",
        "print(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD_i2a6W5Cbc"
      },
      "outputs": [],
      "source": [
        "zip = ZipFile(dataset_path)\n",
        "\n",
        "# Export csv file containing segmentations\n",
        "csv_path = Path(zip.extract('train_ship_segmentations_v2.csv', raw_dir))\n",
        "segmentations = pd.read_csv(csv_path)\n",
        "segmentations['EncodedPixels'] = segmentations['EncodedPixels'].fillna('')\n",
        "segmentations = segmentations.groupby('ImageId').agg({'EncodedPixels': ' '.join})\n",
        "\n",
        "print(f'There are {len(segmentations)} number of images in the dataset')\n",
        "segmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxdkiXiF5Cbc"
      },
      "outputs": [],
      "source": [
        "imgs_with_ships = segmentations[segmentations['EncodedPixels'] != '']\n",
        "print(f'There are {len(imgs_with_ships)} images that contain ships')\n",
        "\n",
        "ratio = len(imgs_with_ships) / len(segmentations)\n",
        "print(f'Ratio of images containing ships and all images: {ratio*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bBZ8vf95Cbc"
      },
      "source": [
        "We have a lot of images that do not contain ships so for now they have\n",
        "less value for us. We are going to use 60000 images to create our own\n",
        "dataset (using all the images would be too much for us anyway). In\n",
        "the 60000 images we will put all of the images that contain ships and\n",
        "for the rest we will use images that do not contain ships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WklrNdp-5Cbd"
      },
      "outputs": [],
      "source": [
        "dataset_size = 60_000\n",
        "\n",
        "image_ids = list(imgs_with_ships.index)\n",
        "print(f'{len(image_ids)} number of images contain ships')\n",
        "\n",
        "# Fill the rest of the dataset with images that do not contain ships\n",
        "print(f'{dataset_size-len(image_ids)} number of images do not contain ships')\n",
        "imgs_without_ships = segmentations[segmentations['EncodedPixels'] == '']\n",
        "image_ids.extend(imgs_without_ships[:dataset_size-len(image_ids)].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6fXWiEY5Cbd"
      },
      "outputs": [],
      "source": [
        "# Our next task will be to only extract the images that are in our dataset\n",
        "for image_id in (t := tqdm(image_ids)):\n",
        "    zip.extract(f'train_v2/{image_id}', path=raw_dir)\n",
        "    t.set_description(f'Extracting: {image_id}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3W6Q082XeXP"
      },
      "outputs": [],
      "source": [
        "!echo \"Number of images in raw dataset: $(ls -1 data/raw/train_v2/ | wc -l)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU2vH1VS7vDX"
      },
      "outputs": [],
      "source": [
        "from math import floor\n",
        "\n",
        "train_size = floor(dataset_size * 0.9)\n",
        "val_size = floor(dataset_size * 0.05)\n",
        "test_size = floor(dataset_size * 0.05)\n",
        "print(f'Full size of dataset: {dataset_size}')\n",
        "print(f'\\tTrain set size: {train_size}')\n",
        "print(f'\\tValidation set size: {val_size}')\n",
        "print(f'\\tTest set size: {test_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM-xvETz1SuE"
      },
      "outputs": [],
      "source": [
        "# Split training images into train, val, test sets using images from the raw dataset\n",
        "!mkdir -p data/processed/\n",
        "\n",
        "# Create training dataset\n",
        "!mkdir -p data/processed/train/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" \\\n",
        "    | head -n {train_size} \\\n",
        "    | tr '\\n' '\\0' \\\n",
        "    | xargs -0 cp -t data/processed/train/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/train_ship_segmentations.csv\n",
        "\n",
        "# Create validation dataset\n",
        "!mkdir -p data/processed/val/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" \\\n",
        "    | head -n {val_size} \\\n",
        "    | tr '\\n' '\\0' \\\n",
        "    | xargs -0 cp -t data/processed/val/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/val_ship_segmentations.csv\n",
        "\n",
        "# Create test dataset\n",
        "!mkdir -p data/processed/test/\n",
        "!find data/raw/train_v2/ -name \"*.jpg\" \\\n",
        "    | head -n {test_size} \\\n",
        "    | tr '\\n' '\\0' \\\n",
        "    | xargs -0 cp -t data/processed/test/\n",
        "!cp data/raw/train_ship_segmentations_v2.csv data/processed/test_ship_segmentations.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itDI-T3R1SuF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter missing annotations\n",
        "for dataset_path in ['train', 'val', 'test']:\n",
        "    filter_missing(proc_dir / f'{dataset_path}_ship_segmentations.csv',\n",
        "                   proc_dir / f'{dataset_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uapCp-Yk5Cbd"
      },
      "outputs": [],
      "source": [
        "!echo \"Number of images in dataset: $(find data/processed/*/ -name \"*.jpg\" | wc -l)\"\n",
        "!echo \"Size of dataset on disk: $(du -sh data/processed)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdEA1nzN5Cbd"
      },
      "outputs": [],
      "source": [
        "# Optional save dataset into google drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    !zip -r gdrive/MyDrive/airbus-dataset.zip data/processed/*\n",
        "    drive.flush_and_unmount()\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}